import streamlit as st
import os
from dotenv import load_dotenv
from langchain.document_loaders import UnstructuredWordDocumentLoader, PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain_openai import ChatOpenAI
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory

# ğŸ‘‰ Load environment variables
load_dotenv()

os.environ["LANGCHAIN_API_KEY"] = os.getenv("LANGCHAIN_API_KEY")
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")

# ğŸ‘‰ Load PDF Document
pdf_loader = PyPDFLoader("PDF Sample.pdf")
pdf_docs = pdf_loader.load()

# ğŸ‘‰ Load Word Document (if needed)
docx_loader = UnstructuredWordDocumentLoader("Docs Sample.docx")
docx_docs = docx_loader.load()

# ğŸ‘‰ Combine all documents
all_docs = pdf_docs + docx_docs

# ğŸ‘‰ Split Text into Chunks
splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=30)
split_docs = splitter.split_documents(all_docs)

# ğŸ‘‰ Generate Embeddings & Store in FAISS
embeddings = OpenAIEmbeddings()
vector_store = FAISS.from_documents(split_docs, embeddings)

# ğŸ‘‰ Setup LangChain Retriever & Memory
llm = ChatOpenAI(model_name="gpt-4", temperature=0)
memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)

# ğŸ‘‰ RAG Chain (Conversational Retrieval)
rag_chain = ConversationalRetrievalChain.from_llm(
    llm=llm,
    retriever=vector_store.as_retriever(),
    memory=memory
)

# ğŸ‘‰ Streamlit UI
st.title("ğŸ“š Q&A Chatbot for Document Retrieval")

# ğŸ‘‰ User Query Input
user_query = st.text_input("ğŸ” Ask a question based on the documents:")

if user_query:
    response = rag_chain.run(user_query)
    st.write("### ğŸ¤– Answer:")
    st.write(response)








